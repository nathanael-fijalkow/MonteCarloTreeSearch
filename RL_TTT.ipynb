{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning algorithms for Tic-Tac-Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class State is used to represent a configuration of the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from math import log, sqrt\n",
    "\n",
    "SIZE = 3\n",
    "\n",
    "class State(object):\n",
    "    def __init__(self):\n",
    "        # @data is a SIZE * SIZE array where\n",
    "        # 0 represents an empty position\n",
    "        # 1 represents a cross (symbol for player 1)\n",
    "        # 2 represents a circle (symbol for player 2)\n",
    "        self.data = np.zeros((SIZE, SIZE))\n",
    "        # @player: who's turn it is to play from this state\n",
    "        self.player = 1\n",
    "        self.hash = 0\n",
    "        # @outcome can be\n",
    "        # 1 if Player 1 wins\n",
    "        # 0 if Player 2 wins\n",
    "        # 0.5 if it's a tie\n",
    "        # -1 if the game is not over\n",
    "        # 2 if the outcome has never been computed\n",
    "        self.outcome = 2\n",
    "\n",
    "    # Checks whether the game is over from this state and who won\n",
    "    def compute_outcome(self):\n",
    "        if self.outcome != 2:\n",
    "            return self.outcome\n",
    "        else:\n",
    "            # Checks rows\n",
    "            for i in range(0, SIZE):\n",
    "                if all(x == 1 for x in self.data[i, :]):\n",
    "                    self.outcome = 1\n",
    "                    return 1\n",
    "                if all(x == 2 for x in self.data[i, :]):\n",
    "                    self.outcome = 0\n",
    "                    return 0\n",
    "\n",
    "            # Checks columns\n",
    "            for j in range(0, SIZE):\n",
    "                if all(x == 1 for x in self.data[:, j]):\n",
    "                    self.outcome = 1\n",
    "                    return 1\n",
    "                if all(x == 2 for x in self.data[:, j]):\n",
    "                    self.outcome = 0\n",
    "                    return 0\n",
    "\n",
    "            # Checks diagonals\n",
    "            diag = [self.data[i,i] for i in range(0, SIZE)]\n",
    "            if all(x == 1 for x in diag):\n",
    "                self.outcome = 1\n",
    "                return 1\n",
    "            if all(x == 2 for x in diag):\n",
    "                self.outcome = 0\n",
    "                return 0\n",
    "\n",
    "            anti_diag = [self.data[i,SIZE - 1 - i] for i in range(0, SIZE)]\n",
    "            if all(x == 1 for x in anti_diag):\n",
    "                self.outcome = 1\n",
    "                return 1\n",
    "            if all(x == 2 for x in anti_diag):\n",
    "                self.outcome = 0\n",
    "                return 0\n",
    "\n",
    "            # Checks whether it's a tie\n",
    "            data_all = [self.data[i,j] for i in range(0, SIZE) for j in range(0, SIZE)]\n",
    "            if all(x != 0 for x in data_all):\n",
    "                self.outcome = 0.5\n",
    "                return 0.5\n",
    "\n",
    "            # If we reached this point the game is still going on\n",
    "            self.outcome = -1\n",
    "            return -1\n",
    "\n",
    "    # Prints the board\n",
    "    def print_state(self):\n",
    "        for i in range(0, SIZE):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(0, SIZE):\n",
    "                if self.data[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                elif self.data[i, j] == 2:\n",
    "                    token = 'o'\n",
    "                else:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------')\n",
    "\n",
    "    # Takes a state and returns the full list of moves that are legal moves\n",
    "    def legal_plays(self):\n",
    "        legal = []\n",
    "        for i in range(0, SIZE):\n",
    "            for j in range(0, SIZE):\n",
    "                if self.data[i, j] == 0:\n",
    "                    legal.append((i,j))\n",
    "        return legal\n",
    "\n",
    "    # Actually not useful because hashes are computed recursively\n",
    "    def compute_hash(self):\n",
    "        self.hash = 0\n",
    "        for i in self.data.reshape(SIZE * SIZE):\n",
    "            self.hash = self.hash * 3 + i\n",
    "        return self.hash\n",
    "\n",
    "    # Compute the hash of the state obtained by playing in (i,j)\n",
    "    def update_hash(self, i, j):\n",
    "        return self.hash + 3 ** (SIZE * i + j) * self.player\n",
    "\n",
    "    # Returns a new state obtained by playing in (i,j)\n",
    "    def next_state(self, i, j):\n",
    "        new_state = State()\n",
    "        new_state.data = np.copy(self.data)\n",
    "        new_state.data[i, j] = self.player\n",
    "        new_state.hash = self.update_hash(i,j)\n",
    "        new_state.player = 3 - self.player\n",
    "        return new_state\n",
    "\n",
    "    # Updates the same state by playing in (i,j)\n",
    "    def update_state(self, i, j):\n",
    "        self.data[i, j] = self.player\n",
    "        self.hash = self.update_hash(i,j)\n",
    "        self.player = 3 - self.player\n",
    "        self.outcome = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class Player implements the different algorithms for constructing a strategy.\n",
    "\n",
    "There are two main choices:\n",
    "* what strategy is used during training: 'epsilon-greedy' or 'UCB'\n",
    "* how do we update estimates: 'average' or 'step_size' or 'TD'\n",
    "\n",
    "### Strategies\n",
    "The **epsilon-greedy** strategy plays at random with probability epsilon, and maximises or minimises the current value otherwise.\n",
    "\n",
    "The **UCB (Upper Confidence Bound)** strategy uses the UCB formula which induces a tradeoff between exploration and exploitation. From a state if all available moves have been explored the strategy chooses the move which maximises or minimises the sum of two terms, one being the current value and the other one indicating how precise is the current value estimate. \n",
    "The parameter 'UCB' controls the exploitation.\n",
    "\n",
    "### Estimates updates\n",
    "The **average** update simply computes for a state the average over all plays containing this state of the outcomes.\n",
    "\n",
    "The **step_size** update changes the value using a step size instead.\n",
    "\n",
    "The **TD (Temporal Difference)** update values locally based on the current value estimates of the next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player():\n",
    "    def __init__(self, name = 'Anonymous', strategy ='epsilon-greedy', update_mode = 'average',\n",
    "                 epsilon = 0.2, UCB = 1.5, step_size = 0.1):\n",
    "        self.values = dict()\n",
    "        self.name = name\n",
    "        # @plays counts for each state how many plays included this state\n",
    "        self.plays = dict()\n",
    "\n",
    "        # What strategy are we using during training: 'epsilon-greedy' or 'UCB'\n",
    "        self.strategy = strategy\n",
    "\n",
    "        # How do we update estimates: 'average' or 'step_size' or 'TD'\n",
    "        self.update_mode = update_mode\n",
    "\n",
    "        # Parameters\n",
    "        self.epsilon = epsilon\n",
    "        self.UCB = UCB\n",
    "        self.step_size = step_size\n",
    "\n",
    "    # Takes the state and returns the move to be applied\n",
    "    def play(self, state, verbose = False):\n",
    "        if not state.hash in self.values:\n",
    "            if verbose:\n",
    "                print(\"The player had never seen that state!\")\n",
    "            return random.choice(state.legal_plays())\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"%s player's turn as Player %d.\\nCurrent value: %0.5f\"  % (self.name, state.player, self.values[state.hash]))\n",
    "                print(\"Available moves and their values:\")\n",
    "                print([((i,j),self.values[state.update_hash(i,j)]) for (i,j) in state.legal_plays()\n",
    "                       if state.update_hash(i,j) in self.values])\n",
    "\n",
    "            # For more fun, we randomise over the most interesting moves\n",
    "            if state.player == 1:\n",
    "                evaluated_moves = [(self.values[state.update_hash(i,j)], (i,j)) for (i, j) in state.legal_plays()\n",
    "                                   if state.update_hash(i,j) in self.values]\n",
    "                max_val, _ = max(evaluated_moves)\n",
    "                interesting_moves = [(i, j) for (v,(i, j)) in evaluated_moves if v == max_val]\n",
    "            else:\n",
    "                evaluated_moves = [(self.values[state.update_hash(i,j)], (i,j)) for (i, j) in state.legal_plays()\n",
    "                                   if state.update_hash(i,j) in self.values]\n",
    "                min_val, _ = min(evaluated_moves)\n",
    "                interesting_moves = [(i, j) for (v,(i, j)) in evaluated_moves if v == min_val]\n",
    "            return random.choice(interesting_moves)\n",
    "\n",
    "    # Computes the (exact) values recursively\n",
    "    def solve(self, state = State()):\n",
    "        print(len(self.values))\n",
    "        if state.compute_outcome() != -1:\n",
    "            self.values[state.hash] = state.outcome\n",
    "        else:\n",
    "            if state.player == 1:\n",
    "                current_val = 0\n",
    "                for (i,j) in state.legal_plays():\n",
    "                    next = state.next_state(i,j)\n",
    "                    if not (next.hash in self.values):\n",
    "                        self.solve(next)\n",
    "                    current_val = max(current_val,self.values[next.hash])\n",
    "                self.values[state.hash] = current_val\n",
    "            else:\n",
    "                current_val = 1\n",
    "                for (i,j) in state.legal_plays():\n",
    "                    next = state.next_state(i,j)\n",
    "                    if not (next.hash in self.values):\n",
    "                        self.solve(next)\n",
    "                    current_val = min(current_val,self.values[next.hash])\n",
    "                self.values[state.hash] = current_val\n",
    "\n",
    "    # During training, takes the current state and returns the move to be applied\n",
    "    # The boolean says whether the move was chosen greedily (True) or uniformly at random (False)\n",
    "    def play_during_training(self, state, t):\n",
    "        possible_states = [((i, j), state.update_hash(i, j)) for (i, j) in state.legal_plays()]\n",
    "        # If we have seen all of the legal moves at least once\n",
    "        if all(hash_val in self.plays for ((i, j), hash_val) in possible_states):\n",
    "            if self.strategy == 'epsilon-greedy':\n",
    "            # Play the epsilon-greedy strategy\n",
    "                if random.random() < self.epsilon:\n",
    "                    return random.choice(state.legal_plays())\n",
    "                else:\n",
    "                    if state.player == 1:\n",
    "                        _, (i, j) = max((self.values[hash_val], (i, j)) for ((i, j), hash_val) in possible_states)\n",
    "                    else:\n",
    "                        _, (i, j) = min((self.values[hash_val], (i, j)) for ((i, j), hash_val) in possible_states)\n",
    "\n",
    "            if self.strategy == 'UCB':\n",
    "            # Play the UCB strategy\n",
    "                if state.player == 1:\n",
    "                    _, (i, j) = max(\n",
    "                        (self.values[hash_val] +\n",
    "                         self.UCB * sqrt(log(self.plays[state.hash]) / self.plays[hash_val]), (i, j))\n",
    "                        for ((i, j), hash_val) in possible_states)\n",
    "                else:\n",
    "                    _, (i, j) = min(\n",
    "                        (self.values[hash_val] -\n",
    "                         self.UCB * sqrt(log(self.plays[state.hash]) / self.plays[hash_val]), (i, j))\n",
    "                        for ((i, j), hash_val) in possible_states)\n",
    "            return i, j\n",
    "        else:\n",
    "        # Otherwise choose randomly among unevaluated moves\n",
    "            unevaluated_moves = [(i, j) for (i, j) in state.legal_plays() if\n",
    "                                 not state.update_hash(i,j) in self.plays]\n",
    "            (i, j) = random.choice(unevaluated_moves)\n",
    "            return i, j\n",
    "\n",
    "    def store_new_state(self, state):\n",
    "        if not(state.hash in self.plays):\n",
    "            self.plays[state.hash] = 0\n",
    "            self.values[state.hash] = 0.5\n",
    "\n",
    "    def run_simulation(self, t):\n",
    "        state = State()\n",
    "        state.hash = 0\n",
    "        self.store_new_state(state)\n",
    "        self.plays[state.hash] += 1\n",
    "\n",
    "        # We store the play in a sequence\n",
    "        play = []\n",
    "\n",
    "        while state.compute_outcome() == -1:\n",
    "            (i, j) = self.play_during_training(state,t)\n",
    "            play.append(state.hash)\n",
    "            state.update_state(i, j)\n",
    "            self.store_new_state(state)\n",
    "            self.plays[state.hash] += 1\n",
    "\n",
    "        if self.update_mode == 'average':\n",
    "            # Update using average\n",
    "            self.values[state.hash] = state.outcome\n",
    "            for hash_val in play:\n",
    "                self.values[hash_val] += 1.0 / self.plays[state.hash] * (state.outcome - self.values[hash_val])\n",
    "\n",
    "        if self.update_mode == 'step_size':\n",
    "            # Update using step size\n",
    "            self.values[state.hash] = state.outcome\n",
    "            for hash_val in play:\n",
    "                self.values[hash_val] += self.step_size * (state.outcome - self.values[hash_val])\n",
    "\n",
    "        if self.update_mode == 'TD':\n",
    "            # Update using temporal difference (TD)\n",
    "            next_hash_val = state.hash\n",
    "            self.values[next_hash_val] = state.outcome\n",
    "            for hash_val in reversed(play):\n",
    "                td_error = self.values[next_hash_val] - self.values[hash_val]\n",
    "                self.values[hash_val] += self.step_size * td_error\n",
    "                next_hash_val = hash_val\n",
    "\n",
    "    def train(self, number_simulations, verbose = False, steps = 100):\n",
    "        if verbose:\n",
    "            print(\"\\nStart training of Player %s\" % self.name)\n",
    "        for t in range(1,number_simulations+1):\n",
    "            self.run_simulation(t)\n",
    "            if verbose and t % steps == 0:\n",
    "                print(\"After %d iterations the value of the initial state is %0.5f\" % (t, self.values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class Competition is used to play strategies against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Competition(object):\n",
    "    # Saves the value function\n",
    "    def save_values(self, name, player):\n",
    "        with open('strategy_%s.bin' % name, 'wb') as f:\n",
    "            pickle.dump(player.values, f)\n",
    "\n",
    "    # Loads a value function\n",
    "    def load_values(self, name, player):\n",
    "        with open('strategy_%s.bin' % name, 'rb') as f:\n",
    "            player.values = pickle.load(f)\n",
    "\n",
    "    # Takes two strategies (one for each player), play them against each other once and declare an outcome\n",
    "    # if player_of_interest is 1 or 2, it assumes that the other player is optimal\n",
    "    def play(self, player1, player2, verbose=False, player_of_interest = 0):\n",
    "        state = State()\n",
    "        state.hash = 0\n",
    "\n",
    "        count_opt = 0\n",
    "        play_length = 0\n",
    "        if verbose:\n",
    "            print(\"\\nMatch between Player %s (as Player 1) and Player %s (as Player 2)\"\n",
    "                  % (player1.name, player2.name))\n",
    "\n",
    "        while state.compute_outcome() == -1:\n",
    "            if verbose:\n",
    "                state.print_state()\n",
    "            if state.player == 1:\n",
    "                i, j = player1.play(state, verbose)\n",
    "                if player_of_interest == 1:\n",
    "                    is_optimal = player2.values[state.hash] <= player2.values[state.update_hash(i, j)]\n",
    "                    count_opt += is_optimal\n",
    "                    play_length += 1\n",
    "                if verbose:\n",
    "                    print(\"Player %d chooses (%d,%d)\" % (1, i, j))\n",
    "                    if player_of_interest == 1 and is_optimal:\n",
    "                        print(\"This was an optimal move, the current value is %0.1f\" %\n",
    "                              player2.values[state.update_hash(i, j)])\n",
    "                state.update_state(i, j)\n",
    "            else:\n",
    "                i, j = player2.play(state, verbose)\n",
    "                if player_of_interest == 2:\n",
    "                    is_optimal = player1.values[state.hash] >= player1.values[state.update_hash(i, j)]\n",
    "                    count_opt += is_optimal\n",
    "                    play_length += 1\n",
    "                if verbose:\n",
    "                    print(\"Player %d chooses (%d,%d)\" % (2, i, j))\n",
    "                    if player_of_interest == 2 and is_optimal:\n",
    "                        print(\"This was an optimal move (the current value is %0.1f)\" %\n",
    "                              player1.values[state.update_hash(i, j)])\n",
    "                state.update_state(i, j)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Final state\")\n",
    "            state.print_state()\n",
    "            if state.outcome == 1:\n",
    "                print(\"Player 1 won\")\n",
    "            elif state.outcome == 0:\n",
    "                print(\"Player 2 won\")\n",
    "            else:\n",
    "                print(\"It's a tie!\")\n",
    "        return state.outcome,count_opt,play_length\n",
    "\n",
    "    # Takes two strategies (one for each player) and play them against each other for a number of games\n",
    "    # if player_of_interest is 1 or 2, it assumes that the other player is optimal\n",
    "    def compete(self, player1, player2, games = 500, player_of_interest = 0):\n",
    "        player1_win = 0.0\n",
    "        player2_win = 0.0\n",
    "        count_opt_tot = 0\n",
    "        count_length_tot = 0\n",
    "        for _ in range(games):\n",
    "            outcome,count_opt,play_length = \\\n",
    "                self.play(player1,player2, verbose = False, player_of_interest = player_of_interest)\n",
    "            count_opt_tot += count_opt\n",
    "            count_length_tot += play_length\n",
    "            if outcome == 1:\n",
    "                player1_win += 1\n",
    "            if outcome == 0:\n",
    "                player2_win += 1\n",
    "        print(\"\\nCompetition of Player %s (as Player 1) against Player %s (as Player 2):\"\n",
    "              \"\\n %d plays, Player 1 wins %.02f, Player 2 wins %.02f\"\n",
    "              % (player1.name, player2.name, games, player1_win / games, player2_win / games))\n",
    "        if player_of_interest:\n",
    "            print(\"Player %s played optimal moves %0.2f percent of the time\" %\n",
    "                  (player1.name if player_of_interest == 1 else player2.name,\n",
    "                   count_opt_tot / count_length_tot * 100))\n",
    "\n",
    "    # Checks whether a player ensures ties against another player over a number of games\n",
    "    def ensures_tie(self, player1, player2, games = 50, player_of_interest = 1):\n",
    "        i = 0\n",
    "        while i < games:\n",
    "            # If the player of interest loses, stop\n",
    "            # Reminder:\n",
    "            # If Player 1 loses the outcome is 0\n",
    "            # If Player 2 loses the outcome is 1\n",
    "            outcome,_,_ = self.play(player1,player2, verbose=False)\n",
    "            if outcome == player_of_interest - 1:\n",
    "                return i\n",
    "            i += 1\n",
    "        return games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate all the players (with default parameters).\n",
    "For convenience the value functions are loaded to avoid training each of them again.\n",
    "Uncomment for training them again and saving the value functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start training of Player epsilon-greedy TD\n",
      "After 1000 iterations the value of the initial state is 0.53811\n",
      "After 2000 iterations the value of the initial state is 0.57664\n",
      "After 3000 iterations the value of the initial state is 0.57842\n",
      "After 4000 iterations the value of the initial state is 0.57953\n",
      "After 5000 iterations the value of the initial state is 0.61402\n",
      "After 6000 iterations the value of the initial state is 0.61667\n",
      "After 7000 iterations the value of the initial state is 0.61267\n",
      "After 8000 iterations the value of the initial state is 0.52120\n",
      "After 9000 iterations the value of the initial state is 0.53539\n",
      "After 10000 iterations the value of the initial state is 0.52489\n",
      "\n",
      "Competition of Player epsilon-greedy TD (as Player 1) against Player Optimal (as Player 2):\n",
      " 500 plays, Player 1 wins 0.00, Player 2 wins 0.26\n",
      "Player epsilon-greedy TD played optimal moves 94.56 percent of the time\n",
      "\n",
      "Competition of Player Optimal (as Player 1) against Player epsilon-greedy TD (as Player 2):\n",
      " 500 plays, Player 1 wins 0.05, Player 2 wins 0.00\n",
      "Player epsilon-greedy TD played optimal moves 98.75 percent of the time\n",
      "\n",
      "Match between Player epsilon-greedy TD (as Player 1) and Player Optimal (as Player 2)\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "epsilon-greedy TD player's turn as Player 1.\n",
      "Current value: 0.52489\n",
      "Available moves and their values:\n",
      "[((0, 0), 0.45501975654779014), ((0, 1), 0.530417696486275), ((0, 2), 0.5120105546855273), ((1, 0), 0.5253748449869733), ((1, 1), 0.44145604990124143), ((1, 2), 0.5173430934515724), ((2, 0), 0.514952400936449), ((2, 1), 0.5347043757619838), ((2, 2), 0.39898014369398227)]\n",
      "Player 1 chooses (2,1)\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "Optimal player's turn as Player 2.\n",
      "Current value: 0.50000\n",
      "Available moves and their values:\n",
      "[((0, 0), 1), ((0, 1), 0.5), ((0, 2), 1), ((1, 0), 1), ((1, 1), 0.5), ((1, 2), 1), ((2, 0), 0.5), ((2, 2), 0.5)]\n",
      "Player 2 chooses (2,2)\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "epsilon-greedy TD player's turn as Player 1.\n",
      "Current value: 0.56375\n",
      "Available moves and their values:\n",
      "[((0, 0), 0.4507374185714651), ((0, 1), 0.25165557861328125), ((0, 2), 0.5468895104098598), ((1, 0), 0.4621584555134177), ((1, 1), 0.6831622935440753), ((1, 2), 0.4042140615058343), ((2, 0), 0.40924878723438773)]\n",
      "Player 1 chooses (1,1)\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "Optimal player's turn as Player 2.\n",
      "Current value: 0.50000\n",
      "Available moves and their values:\n",
      "[((0, 0), 1), ((0, 1), 0.5), ((0, 2), 1), ((1, 0), 1), ((1, 2), 1), ((2, 0), 1)]\n",
      "Player 2 chooses (0,1)\n",
      "-------------\n",
      "|   | o |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "epsilon-greedy TD player's turn as Player 1.\n",
      "Current value: 0.60938\n",
      "Available moves and their values:\n",
      "[((0, 0), 0.38640666475239477), ((0, 2), 0.613053511508258), ((1, 0), 0.5701175817957846), ((1, 2), 0.5000930976073952), ((2, 0), 0.25592041015625)]\n",
      "Player 1 chooses (0,2)\n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   | x | o | \n",
      "-------------\n",
      "Optimal player's turn as Player 2.\n",
      "Current value: 0.50000\n",
      "Available moves and their values:\n",
      "[((0, 0), 1), ((1, 0), 1), ((1, 2), 1), ((2, 0), 0.5)]\n",
      "Player 2 chooses (2,0)\n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "epsilon-greedy TD player's turn as Player 1.\n",
      "Current value: 0.61211\n",
      "Available moves and their values:\n",
      "[((0, 0), 0.5), ((1, 0), 0.5000076295037859), ((1, 2), 0.8137655284970151)]\n",
      "Player 1 chooses (1,2)\n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "|   | x | x | \n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "Optimal player's turn as Player 2.\n",
      "Current value: 0.50000\n",
      "Available moves and their values:\n",
      "[((0, 0), 1), ((1, 0), 0.5)]\n",
      "Player 2 chooses (1,0)\n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "| o | x | x | \n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "epsilon-greedy TD player's turn as Player 1.\n",
      "Current value: 0.50000\n",
      "Available moves and their values:\n",
      "[((0, 0), 0.5)]\n",
      "Player 1 chooses (0,0)\n",
      "Final state\n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n",
      "| o | x | x | \n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "It's a tie!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 0, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competition = Competition()\n",
    "\n",
    "player_optimal = Player(name = \"Optimal\")\n",
    "#player_optimal.solve()\n",
    "#competition.save_values(\"optimal\", player_optimal)\n",
    "competition.load_values(\"optimal\", player_optimal)\n",
    "\n",
    "player_eps_average = Player(name='epsilon-greedy average sample', strategy='epsilon-greedy', update_mode='average')\n",
    "player_eps_step_size = Player(name='epsilon-greedy step size', strategy='epsilon-greedy', update_mode='step_size')\n",
    "player_eps_td = Player(name='epsilon-greedy TD', strategy='epsilon-greedy', update_mode='TD', step_size=0.5)\n",
    "\n",
    "player_UCB_average = Player(name='UCB average sample', strategy='UCB', update_mode='average')\n",
    "player_UCB_step_size = Player(name='UCB step size', strategy='UCB', update_mode='step_size')\n",
    "player_UCB_td = Player(name='UCB TD', strategy='UCB', update_mode='TD', step_size=0.5)\n",
    "\n",
    "player_eps_td.train(10000, True, steps = 1000)\n",
    "competition.compete(player_eps_td, player_optimal, player_of_interest=1)\n",
    "competition.compete(player_optimal, player_eps_td, player_of_interest=2)\n",
    "competition.play(player_eps_td,player_optimal,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many iterations to ensure a tie against the optimal player?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Player UCB TD: how many iterations to win when playing second\n",
      "After 1000 iterations, Player UCB TD lost the match number 0\n",
      "After 2000 iterations, Player UCB TD lost the match number 0\n",
      "After 3000 iterations, Player UCB TD lost the match number 0\n",
      "After 4000 iterations, Player UCB TD lost the match number 4\n",
      "After 5000 iterations, Player UCB TD lost the match number 21\n",
      "After 6000 iterations, Player UCB TD lost the match number 1\n",
      "After 7000 iterations, Player UCB TD lost the match number 9\n",
      "Over: after 8000 iterations, Player UCB TD ensured ties each of the 100 matches\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def how_many_iterations(player,steps = 100, games = 500, verbose = False, player_of_interest = 1):\n",
    "    if verbose:\n",
    "        print(\"\\nPlayer %s: how many iterations to win when playing %s\" % (player.name, \"first\" if player_of_interest == 1 else \"second\"))\n",
    "    iteration = 0\n",
    "    while(True):\n",
    "        player.train(steps)\n",
    "        iteration += steps\n",
    "        if player_of_interest == 1:\n",
    "            result = competition.ensures_tie(player,player_optimal,games,player_of_interest)\n",
    "        if player_of_interest == 2:\n",
    "            result = competition.ensures_tie(player_optimal,player,games,player_of_interest)\n",
    "        if result == games:\n",
    "            if verbose:\n",
    "                print(\"Over: after %d iterations, Player %s ensured ties each of the %d matches\"\n",
    "                      % (iteration, player.name,games))\n",
    "            return iteration\n",
    "        elif verbose:\n",
    "            print(\"After %d iterations, Player %s lost the match number %d\" % (iteration, player.name, result))\n",
    "\n",
    "#how_many_iterations(player_eps_average, steps = 1000, games = 100, verbose = True, player_of_interest = 2)\n",
    "#how_many_iterations(player_eps_step_size, steps = 1000, games = 100, verbose = True, player_of_interest = 2)\n",
    "#how_many_iterations(player_eps_td, steps = 1000, games = 100, verbose = True, player_of_interest = 1)\n",
    "\n",
    "#how_many_iterations(player_UCB_average, steps = 1000, games = 100, verbose = True, player_of_interest = 1)\n",
    "#how_many_iterations(player_UCB_step_size, steps = 1000, games = 100, verbose = True, player_of_interest = 1)\n",
    "how_many_iterations(player_UCB_td, steps = 1000, games = 100, verbose = True, player_of_interest = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore the influence of the different parameters for each of the players.\n",
    "Running the statistics takes a long time since we train a lot of players!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def statistics(strategy, update_mode, parameter_name, parameter_list, number_tests, steps, games):\n",
    "    out = [[] for i in range(len(parameter_list))]\n",
    "    print(\"Statistics for the parameter %s\" % parameter_name)\n",
    "    for index,parameter in enumerate(parameter_list):\n",
    "        print(\"parameter: %0.2f\" % parameter)\n",
    "        for i in range(number_tests):\n",
    "            if parameter_name == 'epsilon':\n",
    "                player = Player(strategy=strategy, update_mode=update_mode, epsilon=parameter)\n",
    "            if parameter_name == 'UCB':\n",
    "                player = Player(strategy=strategy, update_mode=update_mode, UCB=parameter)\n",
    "            if parameter_name == 'step size':\n",
    "                player = Player(strategy=strategy, update_mode=update_mode, step_size=parameter)\n",
    "            out[index].append(how_many_iterations(player,steps,games,verbose=False))\n",
    "    with open('statistics_%s_%s_%s.bin' % (strategy, update_mode, parameter_name), 'wb') as f:\n",
    "        pickle.dump(out, f)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.violinplot(out,parameter_list,widths=0.03)\n",
    "    ax.set_xlabel(\"Parameter: %s\" %parameter_name)\n",
    "    ax.set_ylabel(\"Number of iterations\")\n",
    "    plt.savefig('statistics_%s_%s_%s.png' % (strategy, update_mode, parameter_name))\n",
    "    plt.close()\n",
    "\n",
    "parameter_list_eps_td = np.arange(0.05,0.55,step = 0.05)\n",
    "#statistics(strategy='epsilon-greedy', update_mode='TD', parameter_name = 'epsilon', parameter_list = parameter_list_eps_td, number_tests = 50, steps = 250, games = 50)\n",
    "\n",
    "parameter_list_ucb_td = [1 + (i + 1) / 10 for i in range(10)]\n",
    "#statistics(strategy='UCB', update_mode='TD', parameter_name = 'UCB', parameter_list = parameter_list_ucb_td, number_tests = 50, steps = 250, games = 50)\n",
    "\n",
    "parameter_list_eps_step_size = np.arange(0.05,0.3,step = 0.05)\n",
    "#statistics(strategy='epsilon-greedy', update_mode='TD', parameter_name = 'step size', parameter_list = parameter_list_eps_step_size, number_tests = 50, steps = 250, games = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
